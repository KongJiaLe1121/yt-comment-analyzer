{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7186838a-4388-4210-ab80-3b9c1625a047",
   "metadata": {},
   "source": [
    "# 04 — Summarizer fine-tune with LoRA (DistilBART)\n",
    "\n",
    "We’ll adapt a compact English summarizer (`sshleifer/distilbart-cnn-12-6`) using **LoRA** so training is fast and light.\n",
    "Inputs are `sum_train.csv`, `sum_val.csv`, `sum_test.csv` containing columns: `doc`, `summary`.\n",
    "We’ll evaluate with **ROUGE** and save a LoRA adapter for your app.\n",
    "\n",
    "---\n",
    "\n",
    "## A) Setup & load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f162b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.55.2\n",
      "torch: 2.8.0+cpu\n",
      "rows: 1 1 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>doc</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>iV46TJKL8cU</td>\n",
       "      <td>I'll give Disney some credit. They are brave e...</td>\n",
       "      <td>I'll give Disney some credit. They are brave e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                                                doc  \\\n",
       "0  iV46TJKL8cU  I'll give Disney some credit. They are brave e...   \n",
       "\n",
       "                                             summary  \n",
       "0  I'll give Disney some credit. They are brave e...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell A1 — Imports & paths\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch, transformers, evaluate\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "\n",
    "# Paths (adjust if your notebook lives elsewhere)\n",
    "DATA_DIR = Path(\"../data\")\n",
    "OUT_DIR  = Path(\"..\") / \"models\" / \"summarizer_lora_bart\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# CSVs expected: columns -> doc, summary\n",
    "train_csv = DATA_DIR / \"sum_train.csv\"\n",
    "val_csv   = DATA_DIR / \"sum_val.csv\"\n",
    "test_csv  = DATA_DIR / \"sum_test.csv\"\n",
    "\n",
    "df_tr = pd.read_csv(train_csv)\n",
    "df_va = pd.read_csv(val_csv)\n",
    "df_te = pd.read_csv(test_csv)\n",
    "\n",
    "print(\"rows:\", len(df_tr), len(df_va), len(df_te))\n",
    "display(df_tr.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179fb7a9-e3ee-4cf9-b039-786656b60c09",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## B) Tokenizer & datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "410e19ea-54a9-470c-8233-9d2f9db9f96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer (DistilBART, English)\n",
    "BASE = \"sshleifer/distilbart-cnn-12-6\"   # compact, English summarizer\n",
    "tok  = AutoTokenizer.from_pretrained(BASE)\n",
    "\n",
    "max_src = 512\n",
    "max_tgt = 128\n",
    "\n",
    "def preprocess(batch):\n",
    "    # encode inputs\n",
    "    model_inputs = tok(batch[\"doc\"], max_length=max_src, truncation=True)\n",
    "    # encode targets\n",
    "    with tok.as_target_tokenizer():\n",
    "        labels = tok(batch[\"summary\"], max_length=max_tgt, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a847f5c-a627-419b-bc5c-d55391de61db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a50cb839c774adaa74953ce605a0e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\My Github Profile\\yt-comment-analyzer\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4006: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1a3228f6e2445e9f6476104e76c4fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fda457d62454665a1e993b9498ffff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build HF datasets\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_tr[[\"doc\",\"summary\"]]),\n",
    "    \"val\":   Dataset.from_pandas(df_va[[\"doc\",\"summary\"]]),\n",
    "    \"test\":  Dataset.from_pandas(df_te[[\"doc\",\"summary\"]]),\n",
    "}).map(preprocess, batched=True, remove_columns=[\"doc\",\"summary\"])\n",
    "\n",
    "# Keep raw references for manual evaluation later\n",
    "val_docs = df_va[\"doc\"].tolist()\n",
    "val_refs = df_va[\"summary\"].tolist()\n",
    "test_docs = df_te[\"doc\"].tolist()\n",
    "test_refs = df_te[\"summary\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04774454-80d6-415d-8471-988ba4032cfb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## C) Base model + LoRA adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38727ec9-e22e-4aeb-b60d-5c8ac007d7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell C1 — Load base model\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c8b3218-8ff6-41f3-8894-9dd96d96c62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected projection modules: ['k_proj', 'out_proj', 'q_proj', 'v_proj']\n",
      "trainable params: 3,145,728 || all params: 308,656,128 || trainable%: 1.0192\n",
      "Trainable params: 3,145,728 / 308,656,128 (1.019%)\n"
     ]
    }
   ],
   "source": [
    "# Auto-detect attention projection module names & attach LoRA\n",
    "def find_attn_proj_names(model):\n",
    "    names = set()\n",
    "    for n, m in model.named_modules():\n",
    "        # Typical BART proj leaves: q_proj, k_proj, v_proj, out_proj\n",
    "        leaf = n.split(\".\")[-1]\n",
    "        if leaf in {\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\"}:\n",
    "            names.add(leaf)\n",
    "    return sorted(names)\n",
    "\n",
    "present = find_attn_proj_names(base)\n",
    "print(\"Detected projection modules:\", present)\n",
    "assert present, \"No attention projection modules found. Model layout unexpected.\"\n",
    "\n",
    "lora = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=present,\n",
    "    task_type=\"SEQ_2_SEQ_LM\",\n",
    ")\n",
    "model = get_peft_model(base, lora)\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Show trainable params to confirm LoRA is active\n",
    "model.print_trainable_parameters()\n",
    "n_all = sum(p.numel() for p in model.parameters())\n",
    "n_train = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable params: {n_train:,} / {n_all:,} ({100*n_train/max(1,n_all):.3f}%)\")\n",
    "assert n_train > 0, \"LoRA did not attach: 0 trainable params.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7994d6-219c-4eb8-bfa5-2afbea83f69c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## D) Training setup (compat-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd893755-5314-4d94-a1ec-55184fcb55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TrainingArguments (CPU/GPU friendly)\n",
    "args = TrainingArguments(\n",
    "    output_dir=str(OUT_DIR / \"ckpt\"),\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    dataloader_pin_memory=False,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a8a8c45-77fd-4ab9-9050-e8da8224f9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kongj\\AppData\\Local\\Temp\\ipykernel_27744\\2000090689.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"val\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tok, model=model),\n",
    "    tokenizer=tok\n",
    ")\n",
    "\n",
    "# Sanity check: loss must require grad\n",
    "batch = next(iter(trainer.get_train_dataloader()))\n",
    "model.train()\n",
    "out = model(\n",
    "    input_ids=batch[\"input_ids\"],\n",
    "    attention_mask=batch[\"attention_mask\"],\n",
    "    labels=batch[\"labels\"],\n",
    ")\n",
    "print(\"Loss requires_grad:\", out.loss.requires_grad)  # must be True\n",
    "assert out.loss.requires_grad, \"Loss has no grad; LoRA might not be attached or model is frozen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07d6c2b-bd2c-4d2a-812f-a52879c09570",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## E) Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a540a7e8-19b8-4e15-903d-d9f4241ae6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:11, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5, training_loss=0.407194709777832, metrics={'train_runtime': 14.0808, 'train_samples_per_second': 0.355, 'train_steps_per_second': 0.355, 'total_flos': 3918098595840.0, 'train_loss': 0.407194709777832, 'epoch': 5.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7fe1798-93f1-46c2-9fc7-dc3115fa3815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE vs VAL: {'rouge1': np.float64(0.5287), 'rouge2': np.float64(0.3372), 'rougeL': np.float64(0.3103), 'rougeLsum': np.float64(0.3103)}\n"
     ]
    }
   ],
   "source": [
    "# Baseline (no LoRA): fair comparison using identical decoding.\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "base_id = \"sshleifer/distilbart-cnn-12-6\"\n",
    "tok0 = AutoTokenizer.from_pretrained(base_id)\n",
    "m0   = AutoModelForSeq2SeqLM.from_pretrained(base_id).eval()\n",
    "\n",
    "def gen_base(texts, bs=4, max_in=max_src, max_out=max_tgt):\n",
    "    outs=[]\n",
    "    for i in range(0, len(texts), bs):\n",
    "        enc = tok0(texts[i:i+bs], return_tensors=\"pt\", truncation=True, padding=True, max_length=max_in)\n",
    "        with torch.no_grad():\n",
    "            g = m0.generate(\n",
    "                **enc,\n",
    "                max_length=max_out,\n",
    "                min_length=int(max_out*0.7),\n",
    "                num_beams=4,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        outs += tok0.batch_decode(g, skip_special_tokens=True)\n",
    "    return outs\n",
    "\n",
    "base_preds = gen_base(val_docs)\n",
    "import evaluate; rouge = evaluate.load(\"rouge\")\n",
    "print(\"BASE vs VAL:\", {k: round(v,4) for k,v in rouge.compute(predictions=base_preds, references=val_refs).items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fc0999-e07f-40b8-a62e-e8cfd9797704",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## F) Manual evaluation (ROUGE) with generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "423b987a-6818-4f7b-aa73-00eefe90e8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE scorer\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccd06695-c690-4f14-9a8c-474b89f01a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch generation helper\n",
    "def generate_batch(texts, batch_size=4, max_in=512, max_out=128, num_beams=4):\n",
    "    preds = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        chunk = texts[i:i+batch_size]\n",
    "        enc = tok(chunk, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_in)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **enc,\n",
    "                max_length=max_out,\n",
    "                min_length=int(max_out * 0.7),\n",
    "                num_beams=num_beams,\n",
    "                no_repeat_ngram_size=3,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        preds.extend(tok.batch_decode(out, skip_special_tokens=True))\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f723bc9-49e3-4c86-a8e6-cc7e5e222475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\My Github Profile\\yt-comment-analyzer\\.venv\\lib\\site-packages\\transformers\\generation\\utils.py:1733: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed in v5. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL ROUGE: {'rouge1': np.float64(0.3952), 'rouge2': np.float64(0.1818), 'rougeL': np.float64(0.2156), 'rougeLsum': np.float64(0.2156)}\n",
      "TEST ROUGE: {'rouge1': np.float64(0.4828), 'rouge2': np.float64(0.2907), 'rougeL': np.float64(0.3103), 'rougeLsum': np.float64(0.3103)}\n"
     ]
    }
   ],
   "source": [
    "# Compute ROUGE on val/test\n",
    "val_preds  = generate_batch(val_docs, batch_size=4, max_in=max_src, max_out=max_tgt, num_beams=4)\n",
    "test_preds = generate_batch(test_docs, batch_size=4, max_in=max_src, max_out=max_tgt, num_beams=4)\n",
    "\n",
    "val_scores  = rouge.compute(predictions=val_preds,  references=val_refs)\n",
    "test_scores = rouge.compute(predictions=test_preds, references=test_refs)\n",
    "\n",
    "print(\"VAL ROUGE:\", {k: round(v,4) for k,v in val_scores.items()})\n",
    "print(\"TEST ROUGE:\", {k: round(v,4) for k,v in test_scores.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2cb4c5-20bf-4913-8a45-981c9dea2881",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## G) Save adapter + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1c30644-531e-4e34-b46e-6bd228371dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved adapter to: ..\\models\\summarizer_lora_bart\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA adapter + tokenizer\n",
    "trainer.save_model(str(OUT_DIR))\n",
    "tok.save_pretrained(str(OUT_DIR))\n",
    "print(\"Saved adapter to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27235096-c199-4236-be44-e1ed0df95492",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## H) Inference helper (drop into your repo)\n",
    "\n",
    "> Copy this into `ml/src/infer_summary.py` or similar so your app can load the adapter easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77c788d1-0330-4fd0-a773-0e7b350582d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference helper\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "\n",
    "class Summarizer:\n",
    "    def __init__(self,\n",
    "                 base=\"sshleifer/distilbart-cnn-12-6\",\n",
    "                 adapter_path=\"../../models/summarizer_lora_bart\",\n",
    "                 max_in=512, max_out=128, num_beams=4):\n",
    "        self.tok = AutoTokenizer.from_pretrained(base)\n",
    "        base_model = AutoModelForSeq2SeqLM.from_pretrained(base)\n",
    "        self.model = PeftModel.from_pretrained(base_model, adapter_path).eval()\n",
    "        self.max_in, self.max_out, self.num_beams = max_in, max_out, num_beams\n",
    "\n",
    "    def __call__(self, text: str):\n",
    "        enc = self.tok([text], truncation=True, padding=True, max_length=self.max_in, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            out = self.model.generate(\n",
    "                **enc, max_length=self.max_out, num_beams=self.num_beams\n",
    "            )\n",
    "        return self.tok.batch_decode(out, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091778ef-09b7-49c2-92b4-b8a45c71a289",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Wrap-up: What we just did (04_summarizer_finetune_peft)\n",
    "\n",
    "**TL;DR:** We fine-tuned a compact BART summarizer using **LoRA** (fast, low-VRAM), evaluated with **ROUGE**, and saved the adapter so your Streamlit app can load it on top of the base model.\n",
    "\n",
    "**Outputs**\n",
    "\n",
    "* Adapter + tokenizer at: `models/summarizer_lora_bart/`\n",
    "\n",
    "**Why LoRA?**\n",
    "\n",
    "* Much smaller to train/save, quicker iterations, and easy to swap on top of a stable base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bbc3f4-9624-4db3-88e8-478ef302e37c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt-comment-venv",
   "language": "python",
   "name": "ig-comment-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
